## 第四章 表示学习

**语义鸿沟：**

低层次特征与高层次抽象特征之间的差异

好的表示需要尽可能的描述一些高层次的抽象特征，以便后续模型可以高效利用这个特征。（换句话说，好的表示是为了减小语义鸿沟）

**表示学习的任务：**

学习这样一个映射
$$
f:X\rightarrow R^d
$$
即将输入映射到一个稠密的低维向量空间中。



**两种典型的表示学习方法：1.基于重构损失的方法 2.基于对比损失的方法**



### 4.2 基于重构损失的方法——自编码器

自编码器（无监督学习模型）

基本思路：将输入映射到某个特征空间，再从这个特征空间映射回输入空间进行重构。

**（由编码器和解码器组成）**

自编码器有欠完备自编码器和过完备自编码器，两者通过$d、n$维度的大小关系来确定。

欠完备自编码器在一定条件下可以得到PCA的效果。

#### 4.2.1 正则化自编码器

**1.去噪自编码器**

在原始输入的基础上加入噪音作为编码器的输出（具体做法是将输入x的一部分值置零）

**2.稀疏自编码器**

给损失函数加上正则项

通过限制神经元的活跃度来约束模型，尽可能使得大多数神经元都处于不活跃的状态。



#### 4.2.2 变分自编码器

本质是生成模型（生成新的样本数据）



### 4.3 基于对比损失的方法——Word2vec

核心思想：用一个词的上下文去刻画这个词。

可以得到不同的两个模型：

**CBow**

给定某个中心词的上下文去预测该中心词；

**Skip-gram**

给定中心词来预测上下文词

