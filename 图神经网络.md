## 第四章 表示学习

**语义鸿沟：**

低层次特征与高层次抽象特征之间的差异

好的表示需要尽可能的描述一些高层次的抽象特征，以便后续模型可以高效利用这个特征。（换句话说，好的表示是为了减小语义鸿沟）

**表示学习的任务：**

学习这样一个映射
$$
f:X\rightarrow R^d
$$
即将输入映射到一个稠密的低维向量空间中。



**两种典型的表示学习方法：1.基于重构损失的方法 2.基于对比损失的方法**



### 4.2 基于重构损失的方法——自编码器

自编码器（无监督学习模型）

基本思路：将输入映射到某个特征空间，再从这个特征空间映射回输入空间进行重构。

**（由编码器和解码器组成）**

自编码器有欠完备自编码器和过完备自编码器，两者通过$d、n$维度的大小关系来确定。

欠完备自编码器在一定条件下可以得到PCA的效果。

#### 4.2.1 正则化自编码器

**1.去噪自编码器**

在原始输入的基础上加入噪音作为编码器的输出（具体做法是将输入x的一部分值置零）

**2.稀疏自编码器**

给损失函数加上正则项

通过限制神经元的活跃度来约束模型，尽可能使得大多数神经元都处于不活跃的状态。



#### 4.2.2 变分自编码器

本质是生成模型（生成新的样本数据）



### 4.3 基于对比损失的方法——Word2vec

核心思想：用一个词的上下文去刻画这个词。

可以得到不同的两个模型：

**CBow**

给定某个中心词的上下文去预测该中心词；

**Skip-gram**

给定中心词来预测上下文词

***

### 第五章 图信号处理与图卷积神经网络

图信号是定义在节点上的信号，节点之间有自己固有的关联结构。

研究图信号的性质，除了要考虑图信号的强度还要考虑图的拓扑结构，不同图上同一强度的信号有截然不同的性质。

***

### 第6章 GCN的性质

#### 6.1 GCN与CNN的联系

本质上看，二者都是聚合邻域信息的运算，只是作用的数据对象不同。

1.图像是一种特殊的图数据

2.从连接方式看，二者都是局部连接，对于拟合能力来说，CNN更有优势。

3.二者卷积核的权重是处处共享的；

4.从模型层面来说，感受域随着卷积层的增加而变大；

**GCN主要任务：**

图分类和节点分类





图数据中包含两部分信息：属性信息和结构信息。

属性信息描述了图中对象的固有性质；

结构信息描述了对象之间的关联性质；



两种典型的图数据学习方式：基于手工特征和基于随机游走的方法

两种方法的问题：节点的特征向量一旦被拼接就会被固化下来，下游任务学习中产生的监督信号不能有效指导图数据的表示学习，高效性降低。相反，GCN进将节点的表示学习和下游的任务学习被放到一个模型里面进行**端到端学习，**

同时，GCN对结构信息和属性信息的学习是同时进行的，这两者有很好的互补关系。



#### 6.3 GCN是一个低通滤波器

图的半监督学习任务中，通常在相应的损失函数里面加上正则项（为了保证相邻节点之间的类别信息趋于一致。）

#### 6.4 GCN的问题——过平滑

不能像CNN那样堆叠的很深，使用多层GCN学习，相关的任务效果就会极具下降。



GCN的本质是聚合邻居信息，随着网络层数的增加，聚合半径不断增加，自身节点所聚合的邻居节点越来越多，可能最终会导致覆盖所有的全图节点，而节点自身的信息就会变少，
